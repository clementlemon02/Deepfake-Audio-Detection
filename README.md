#Deepfake Audio Detection Project
Overview
This repository presents a deepfake audio detection system that leverages five distinct machine learning models to identify manipulated audio recordings. The primary goal of this project is to evaluate the effectiveness of different model architectures for the task of deepfake detection, aiming to provide robust and accurate results. The models included in this project are RNN, CNN, Transformer, wav2vec, and RawNet2.

Features
Five Independent Models:
RNN (Recurrent Neural Network): Captures sequential dependencies in audio data.
CNN (Convolutional Neural Network): Extracts local features from audio spectrograms.
Transformer: Utilizes self-attention mechanisms for contextual audio analysis.
wav2vec: Applies self-supervised learning for robust feature extraction.
RawNet2: Processes raw audio data directly for end-to-end deepfake detection.
Audio Representations: Models can process CQT spectrograms, log spectrograms, and raw audio formats.
Comprehensive Evaluation Metrics: Reports accuracy, F1-score, AUC (Area Under the Curve), and EER (Equal Error Rate).








